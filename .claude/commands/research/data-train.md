---
description: 'ML è½½è·è¯†åˆ«è®­ç»ƒ - æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°'
---

# /data:train - ML è®­ç»ƒ

$ARGUMENTS

---

## ä¾èµ–èƒ½åŠ›

- `ml.train` - æ¨¡å‹è®­ç»ƒ (ç”¨æˆ·é…ç½®)
- `data.load` - æ•°æ®åŠ è½½ (ç”¨æˆ·é…ç½®)
- `data.plot` - å¯è§†åŒ– (ç”¨æˆ·é…ç½®)
- `docs.query` - scikit-learn/PyTorch æ–‡æ¡£ (context7)

## æ‰§è¡Œæµç¨‹

### 1. ä¸Šä¸‹æ–‡æ£€ç´¢
- è°ƒç”¨ `mcp__ace-tool__search_context` æ£€æŸ¥è®­ç»ƒæ•°æ®å’Œå·²æœ‰æ¨¡å‹

### 2. å‚æ•°æ ¡éªŒ
æ£€æŸ¥è¾“å…¥æ˜¯å¦åŒ…å«:
- **è®­ç»ƒæ•°æ®**: CSV, NPY, HDF5
- **ç›®æ ‡å˜é‡**: åˆ—åæˆ–ç´¢å¼•
- **æ¨¡å‹ç±»å‹**: RF, XGBoost, LSTM, CNN-LSTM, Transformer
- **è®­ç»ƒé…ç½®**: train/test åˆ†å‰²ã€äº¤å‰éªŒè¯ã€è¶…å‚æ•°

### 3. è®¡åˆ’é¢„è§ˆ

```markdown
## ğŸ“‹ æ‰§è¡Œè®¡åˆ’

| æ­¥éª¤ | æ“ä½œ | é£é™©ç­‰çº§ |
|------|------|----------|
| 1 | åŠ è½½æ•°æ® + è·å– ML æ–‡æ¡£ | low |
| 2 | æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹ | low |
| 3 | Codex ç”Ÿæˆè®­ç»ƒä»£ç  | medium |
| 4 | æ‰§è¡Œè®­ç»ƒ + è¯„ä¼° | high |
| 5 | ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š | low |

**é¢„è®¡å½±å“**:
- ç”Ÿæˆæ¨¡å‹æ–‡ä»¶: `artifacts/models/[model-name].pkl`
- ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š: `artifacts/reports/training-report.md`
- å¯èƒ½æ¶ˆè€—å¤§é‡è®¡ç®—èµ„æº
```

### 4. ç¡®è®¤é—¨æ§
- **é£é™©ç­‰çº§**: high (æ¶ˆè€—è®¡ç®—èµ„æºã€å¯èƒ½é•¿æ—¶é—´è¿è¡Œ)
- **è¡Œä¸º**: confirm + preview - å±•ç¤ºè®¡åˆ’ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤

### 5. æ‰§è¡Œ

#### Step 1: é¢„æ£€ç´¢ ML æ–‡æ¡£
ä½¿ç”¨ context7 è·å–è®­ç»ƒæœ€ä½³å®è·µ:
```javascript
mcp__context7__resolve-library-id({
  libraryName: "scikit-learn",
  query: "random forest, cross validation, hyperparameter tuning"
})

mcp__context7__query-docs({
  libraryId: "/scikit-learn/scikit-learn",
  query: "RandomForestRegressor, GridSearchCV, train_test_split examples"
})
```

#### Step 2: Codex ç”Ÿæˆè®­ç»ƒä»£ç 

```bash
C:/Users/ljh/.claude/bin/codeagent-wrapper.exe --backend codex - "$PWD" <<'EOF'
ROLE_FILE: C:/Users/ljh/.claude/.ccg/prompts/codex/architect.md
<TASK>
ä»»åŠ¡ç±»å‹: ML æ¨¡å‹è®­ç»ƒè„šæœ¬
éœ€æ±‚: [ç”¨æˆ·è®­ç»ƒéœ€æ±‚ï¼Œå¦‚è¦†å†°è½½è·é¢„æµ‹]
ä¸Šä¸‹æ–‡:
- è®­ç»ƒæ•°æ®: [data-file-path]
- ç›®æ ‡å˜é‡: [target-column]
- ç‰¹å¾: [feature-columns]
- æ¨¡å‹ç±»å‹: [RandomForest / XGBoost / LSTM]
- ML æ–‡æ¡£ (from context7): [è®­ç»ƒæœ€ä½³å®è·µ]
ç”Ÿæˆå†…å®¹:
1. æ•°æ®åŠ è½½ä¸æ¢ç´¢
2. ç‰¹å¾å·¥ç¨‹ (æ ‡å‡†åŒ–ã€ç¼–ç ã€æ—¶åºç‰¹å¾)
3. æ•°æ®åˆ†å‰² (train/val/test)
4. æ¨¡å‹å®šä¹‰ä¸è®­ç»ƒ
5. è¶…å‚æ•°è°ƒä¼˜ (GridSearchCV / Optuna)
6. æ¨¡å‹è¯„ä¼° (RMSE, MAE, RÂ²)
7. æ¨¡å‹ä¿å­˜
8. è®­ç»ƒå¯è§†åŒ– (å­¦ä¹ æ›²çº¿ã€ç‰¹å¾é‡è¦æ€§)
</TASK>
OUTPUT: å®Œæ•´ Python è„šæœ¬
æ³¨æ„äº‹é¡¹:
1. æ·»åŠ è¿›åº¦æ—¥å¿—
2. æ”¯æŒæ—©åœ (early stopping)
3. ä¿å­˜æœ€ä½³æ¨¡å‹
4. è®°å½•è¶…å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡
EOF
```

Codex è¿”å›ç¤ºä¾‹è„šæœ¬:
```python
#!/usr/bin/env python
# train_ice_load_model.py
# Generated by /data:train

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import matplotlib.pyplot as plt

# é…ç½®
DATA_FILE = 'sensor_data_processed.csv'
TARGET_COL = 'ice_load'
FEATURE_COLS = ['strain_1', 'strain_2', 'strain_3', 'temp', 'wind_speed']
MODEL_OUTPUT = 'artifacts/models/ice_load_rf.pkl'
RANDOM_STATE = 42

# åŠ è½½æ•°æ®
print("Loading data...")
df = pd.read_csv(DATA_FILE)
X = df[FEATURE_COLS]
y = df[TARGET_COL]

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è¶…å‚æ•°æœç´¢
print("\nHyperparameter tuning...")
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)
grid_search = GridSearchCV(
    rf, param_grid, cv=5, scoring='neg_mean_squared_error',
    n_jobs=-1, verbose=1
)
grid_search.fit(X_train_scaled, y_train)

print(f"Best parameters: {grid_search.best_params_}")
best_model = grid_search.best_estimator_

# è¯„ä¼°
y_pred = best_model.predict(X_test_scaled)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\n=== Evaluation Results ===")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ²: {r2:.4f}")

# äº¤å‰éªŒè¯
cv_scores = cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='r2')
print(f"CV RÂ² scores: {cv_scores}")
print(f"CV RÂ² mean: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")

# ä¿å­˜æ¨¡å‹
joblib.dump({'model': best_model, 'scaler': scaler}, MODEL_OUTPUT)
print(f"\nModel saved to {MODEL_OUTPUT}")

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# é¢„æµ‹ vs å®é™…
axes[0, 0].scatter(y_test, y_pred, alpha=0.5)
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
axes[0, 0].set_xlabel('Actual')
axes[0, 0].set_ylabel('Predicted')
axes[0, 0].set_title(f'Prediction vs Actual (RÂ²={r2:.4f})')

# æ®‹å·®åˆ†å¸ƒ
residuals = y_test - y_pred
axes[0, 1].hist(residuals, bins=50, edgecolor='black')
axes[0, 1].set_xlabel('Residual')
axes[0, 1].set_title('Residual Distribution')

# ç‰¹å¾é‡è¦æ€§
importance = best_model.feature_importances_
axes[1, 0].barh(FEATURE_COLS, importance)
axes[1, 0].set_xlabel('Importance')
axes[1, 0].set_title('Feature Importance')

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve
train_sizes, train_scores, val_scores = learning_curve(
    best_model, X_train_scaled, y_train, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10), scoring='r2'
)
axes[1, 1].plot(train_sizes, train_scores.mean(axis=1), label='Train')
axes[1, 1].plot(train_sizes, val_scores.mean(axis=1), label='Validation')
axes[1, 1].set_xlabel('Training Size')
axes[1, 1].set_ylabel('RÂ² Score')
axes[1, 1].set_title('Learning Curve')
axes[1, 1].legend()

plt.tight_layout()
plt.savefig('artifacts/figures/training-results.png', dpi=300)
print("Training visualization saved")
```

#### Step 3: æ‰§è¡Œè®­ç»ƒ
```bash
python scripts/train_ice_load_model.py
```

#### Step 4: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š

```markdown
# ML è®­ç»ƒæŠ¥å‘Š
**æ¨¡å‹**: RandomForestRegressor
**ä»»åŠ¡**: è¦†å†°è½½è·é¢„æµ‹
**ç”Ÿæˆæ—¶é—´**: 2026-01-17 10:30:00

---

## æ•°æ®é›†
- è®­ç»ƒæ ·æœ¬: 8,000
- æµ‹è¯•æ ·æœ¬: 2,000
- ç‰¹å¾æ•°é‡: 5
- ç›®æ ‡å˜é‡: ice_load

## æœ€ä¼˜è¶…å‚æ•°
| å‚æ•° | å€¼ |
|------|------|
| n_estimators | 200 |
| max_depth | 20 |
| min_samples_split | 5 |
| min_samples_leaf | 2 |

## æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | å€¼ | è¯„ä»· |
|------|------|------|
| RMSE | 0.0234 | ä¼˜ç§€ |
| MAE | 0.0189 | ä¼˜ç§€ |
| RÂ² | 0.9543 | ä¼˜ç§€ |
| CV RÂ² | 0.9521 Â± 0.0123 | ç¨³å®š |

## ç‰¹å¾é‡è¦æ€§
| ç‰¹å¾ | é‡è¦æ€§ |
|------|--------|
| strain_1 | 0.35 |
| strain_2 | 0.28 |
| wind_speed | 0.20 |
| strain_3 | 0.12 |
| temp | 0.05 |

## å¯è§†åŒ–
![](artifacts/figures/training-results.png)

## æ¨¡å‹æ–‡ä»¶
- æ¨¡å‹: `artifacts/models/ice_load_rf.pkl`
- è®­ç»ƒè„šæœ¬: `scripts/train_ice_load_model.py`
```

### 6. ç»“æœå‘ˆç°

```markdown
## âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ

### æ€§èƒ½æ‘˜è¦
- **æ¨¡å‹ç±»å‹**: RandomForestRegressor
- **RÂ² åˆ†æ•°**: 0.9543
- **RMSE**: 0.0234

### ç”Ÿæˆæ–‡ä»¶
- æ¨¡å‹æ–‡ä»¶: `artifacts/models/ice_load_rf.pkl`
- è®­ç»ƒæŠ¥å‘Š: `artifacts/reports/training-report.md`
- å¯è§†åŒ–: `artifacts/figures/training-results.png`

### åç»­æ“ä½œ
- æ¨¡å‹æ¨ç†: `/data:predict --model ice_load_rf.pkl --input new_data.csv`
- å‚æ•°æ ‡å®š: `/data:calibrate --model ice_load_rf.pkl`
- æ¨¡å‹è§£é‡Š: `/data:explain --model ice_load_rf.pkl`
```

---

## æ¨¡å‹ç±»å‹

### ä¼ ç»Ÿ ML
```bash
# éšæœºæ£®æ—
/data:train data.csv --target ice_load --model rf

# XGBoost
/data:train data.csv --target ice_load --model xgboost

# æ”¯æŒå‘é‡æœº
/data:train data.csv --target ice_load --model svr
```

### æ·±åº¦å­¦ä¹ 
```bash
# LSTM
/data:train data.csv --target ice_load --model lstm --seq-length 50

# CNN-LSTM
/data:train data.csv --target ice_load --model cnn-lstm

# Transformer
/data:train data.csv --target ice_load --model transformer
```

### é›†æˆå­¦ä¹ 
```bash
# Stacking
/data:train data.csv --target ice_load --model stacking --base rf,xgb,svr

# Blending
/data:train data.csv --target ice_load --model blending
```

---

## è®­ç»ƒé…ç½®

### æ•°æ®åˆ†å‰²
```bash
/data:train data.csv --split 0.8/0.1/0.1  # train/val/test
/data:train data.csv --cv 5  # 5æŠ˜äº¤å‰éªŒè¯
/data:train data.csv --time-split  # æ—¶åºåˆ†å‰²
```

### è¶…å‚æ•°è°ƒä¼˜
```bash
/data:train data.csv --tune grid  # GridSearchCV
/data:train data.csv --tune random --n-iter 100  # RandomizedSearchCV
/data:train data.csv --tune optuna --trials 200  # Optuna
```

### GPU åŠ é€Ÿ
```bash
/data:train data.csv --gpu  # ä½¿ç”¨ GPU
/data:train data.csv --mixed-precision  # æ··åˆç²¾åº¦è®­ç»ƒ
```

---

## å¤šæ¨¡å‹åä½œ

**åä½œæ¨¡å¼**: Codex ä¸»å¯¼

| æ¨¡å‹ | èŒè´£ | è¾“å‡º |
|------|------|------|
| **Context7** | æä¾› scikit-learn/PyTorch æ–‡æ¡£ | ML æœ€ä½³å®è·µ |
| **Codex** | ç”Ÿæˆè®­ç»ƒä»£ç ã€è°ƒå‚ç­–ç•¥ | Python è„šæœ¬ |
| **Claude** | é¢„æ£€ç´¢æ–‡æ¡£ã€æ‰§è¡Œè®­ç»ƒã€ç”ŸæˆæŠ¥å‘Š | æ¨¡å‹æ–‡ä»¶ + æŠ¥å‘Š |

---

## é”™è¯¯å¤„ç†

| é”™è¯¯ç±»å‹ | å¤„ç†ç­–ç•¥ |
|----------|----------|
| å†…å­˜ä¸è¶³ | å»ºè®®é™é‡‡æ ·æˆ–ä½¿ç”¨å¢é‡å­¦ä¹  |
| è®­ç»ƒå‘æ•£ | é™ä½å­¦ä¹ ç‡ï¼Œæ£€æŸ¥æ•°æ®è´¨é‡ |
| è¿‡æ‹Ÿåˆ | å¢åŠ æ­£åˆ™åŒ–ï¼Œæ—©åœ |
| GPU ä¸å¯ç”¨ | å›é€€åˆ° CPU è®­ç»ƒ |
| ä¾èµ–ç¼ºå¤± | æç¤ºå®‰è£…æ‰€éœ€åŒ… |

---

## å®‰å…¨æœºåˆ¶

1. **èµ„æºé¢„ä¼°**: è®­ç»ƒå‰è¯„ä¼°å†…å­˜å’Œæ—¶é—´éœ€æ±‚
2. **æ£€æŸ¥ç‚¹ä¿å­˜**: å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€
3. **æ—©åœæœºåˆ¶**: éªŒè¯é›†æ€§èƒ½ä¸å†æå‡æ—¶åœæ­¢
4. **è¶…æ—¶ä¿æŠ¤**: è¶…è¿‡é¢„è®¾æ—¶é—´è‡ªåŠ¨ä¸­æ–­
5. **å›æ»šæ”¯æŒ**: ä¿ç•™å†å²æ¨¡å‹ç‰ˆæœ¬

---

## å‚è€ƒ

- å…±äº«åè®®: `.claude/commands/research/_protocol.md`
- èƒ½åŠ›é…ç½®: `.claude/.research/capabilities.yaml`
- æ•°æ®å¤„ç†: `/data:process`
- ä»¿çœŸ-å®æµ‹å¯¹æ¯”: `/data:compare`
- å‚æ•°æ ‡å®š: `/data:calibrate`
